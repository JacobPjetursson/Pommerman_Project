{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an agent in Pommerman\n",
    "\n",
    "This notebook contains all the code necessary for training the model and visualizing it.\n",
    "\n",
    "We have already trained a model, so you can skip straight to the evaluation/visualization part below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import algo\n",
    "\n",
    "from envs import make_vec_envs\n",
    "from models import create_policy\n",
    "from rollout_storage import RolloutStorage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_algo = 'a2c'\n",
    "load = True\n",
    "\n",
    "arg_num_steps = 10\n",
    "arg_num_processes = 1\n",
    "arg_num_frames = 5e7\n",
    "arg_lr_schedule = 500000\n",
    "arg_seed = 1\n",
    "\n",
    "arg_log_dir = '/tmp/gym/'\n",
    "arg_save_dir = '/trained_models/'\n",
    "\n",
    "arg_load_path = './trained_models/a2c/PommeFFACompetitionFast-v0.pt'\n",
    "\n",
    "arg_env_name = 'PommeFFACompetitionFast-v0'\n",
    "\n",
    "arg_lr = 2.5e-4\n",
    "arg_eps = 1e-5\n",
    "arg_alpha = 0.99\n",
    "arg_gamma = 0.9\n",
    "arg_tau = 0.95\n",
    "arg_no_norm = True\n",
    "arg_num_stack = 1\n",
    "arg_clip_param = 0.2\n",
    "\n",
    "arg_value_loss_coef = 0.5\n",
    "arg_loss_coef = 0.5\n",
    "arg_entropy_coef = 0.01\n",
    "\n",
    "arg_use_gae = False\n",
    "\n",
    "arg_max_grad_norm = 0.5\n",
    "arg_log_interval = 10\n",
    "arg_save_interval = 100\n",
    "arg_eval_interval = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important:__ Do you have cuda enabled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    assert arg_algo in ['a2c', 'ppo']\n",
    "\n",
    "    update_factor = arg_num_steps * arg_num_processes\n",
    "    num_updates = int(arg_num_frames) // update_factor\n",
    "    lr_update_schedule = None if arg_lr_schedule is None else arg_lr_schedule // update_factor\n",
    "\n",
    "    torch.manual_seed(arg_seed)\n",
    "    if arg_cuda:\n",
    "        torch.cuda.manual_seed(arg_seed)\n",
    "    np.random.seed(arg_seed)\n",
    "\n",
    "    try:\n",
    "        os.makedirs(arg_log_dir)\n",
    "    except OSError:\n",
    "        files = glob.glob(os.path.join(arg_log_dir, '*.monitor.csv'))\n",
    "        try:\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    eval_log_dir = arg_log_dir + \"_eval\"\n",
    "    try:\n",
    "        os.makedirs(eval_log_dir)\n",
    "    except OSError:\n",
    "        files = glob.glob(os.path.join(eval_log_dir, '*.monitor.csv'))\n",
    "        try:\n",
    "            for f in files:\n",
    "                os.remove(f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    torch.set_num_threads(1)\n",
    "    device = torch.device(\"cuda:0\" if arg_cuda else \"cpu\")\n",
    "\n",
    "    train_envs = make_vec_envs(\n",
    "        arg_env_name, arg_seed, arg_num_processes, arg_gamma, arg_no_norm, arg_num_stack,\n",
    "        arg_log_dir, device, allow_early_resets=False)\n",
    "\n",
    "    if arg_eval_interval:\n",
    "        eval_envs = make_vec_envs(\n",
    "            arg_env_name, arg_seed + arg_num_processes, arg_num_processes, arg_gamma,\n",
    "            arg_no_norm, arg_num_stack, eval_log_dir, device,\n",
    "            allow_early_resets=True, eval=True)\n",
    "\n",
    "        if eval_envs.venv.__class__.__name__ == \"VecNormalize\":\n",
    "            eval_envs.venv.ob_rms = train_envs.venv.ob_rms\n",
    "    else:\n",
    "        eval_envs = None\n",
    "\n",
    "    actor_critic = create_policy(\n",
    "        train_envs.observation_space,\n",
    "        nn_kwargs={\n",
    "            'batch_norm': True,\n",
    "            'hidden_size': 512,\n",
    "        },\n",
    "        train=True)\n",
    "    if arg_load_path and load:\n",
    "        print(\"Loading in previous model\")\n",
    "        try:\n",
    "            path_ = \"./trained_models/a2c/PommeFFACompetitionFast-v0.pt\"\n",
    "            if arg_algo.startswith('ppo'):\n",
    "                path_ = \"./trained_models/ppo/PommeFFACompetitionFast-v0.pt\"\n",
    "\n",
    "            state_dict, ob_rms = torch.load(path_)\n",
    "            actor_critic.load_state_dict(state_dict)\n",
    "        except:\n",
    "            print(\"Wrong path!\")\n",
    "            exit(1)\n",
    "    actor_critic.to(device)\n",
    "\n",
    "    if arg_algo.startswith('a2c'):\n",
    "        agent = algo.A2C(\n",
    "            actor_critic, arg_value_loss_coef,\n",
    "            arg_entropy_coef,\n",
    "            lr=arg_lr, lr_schedule=lr_update_schedule,\n",
    "            eps=arg_eps, alpha=arg_alpha,\n",
    "            max_grad_norm=arg_max_grad_norm)\n",
    "    elif arg_algo.startswith('ppo'):\n",
    "        agent = algo.OUR_PPO(  # PPO HER!\n",
    "            actor_critic, arg_clip_param, arg_ppo_epoch, arg_num_mini_batch,\n",
    "            arg_value_loss_coef, arg_entropy_coef,\n",
    "            lr=arg_lr, lr_schedule=lr_update_schedule,\n",
    "            eps=arg_eps,\n",
    "            max_grad_norm=arg_max_grad_norm)\n",
    "\n",
    "    rollouts = RolloutStorage(\n",
    "        arg_num_steps, arg_num_processes,\n",
    "        train_envs.observation_space.shape,\n",
    "        train_envs.action_space)\n",
    "\n",
    "    obs = train_envs.reset()\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(num_updates):\n",
    "        for step in range(arg_num_steps):\n",
    "            # Sample actions\n",
    "            with torch.no_grad():\n",
    "                value, action, action_log_prob = actor_critic.act(\n",
    "                    rollouts.obs[step],\n",
    "                    rollouts.masks[step])\n",
    "\n",
    "            # Obser reward and next obs\n",
    "            obs, reward, done, infos = train_envs.step(action)\n",
    "\n",
    "            for info in infos:\n",
    "                if 'episode' in info.keys():\n",
    "                    rew = info['episode']['r']\n",
    "                    episode_rewards.append(rew)\n",
    "\n",
    "            # If done then clean the history of observations.\n",
    "            masks = torch.tensor([[0.0] if done_ else [1.0] for done_ in done], device=device)\n",
    "            rollouts.insert(obs, action, action_log_prob, value, reward, masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_value = actor_critic.get_value(rollouts.obs[-1],\n",
    "                                                rollouts.masks[-1]).detach()\n",
    "\n",
    "        rollouts.compute_returns(next_value, arg_use_gae, arg_gamma, arg_tau)\n",
    "\n",
    "        value_loss, action_loss, dist_entropy, other_metrics = agent.update(rollouts, j)\n",
    "\n",
    "        rollouts.after_update()\n",
    "\n",
    "        if j % arg_save_interval == 0 and arg_save_dir != \"\":\n",
    "            save_path = os.path.join(arg_save_dir, arg_algo)\n",
    "            try:\n",
    "                os.makedirs(save_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "            # Save model\n",
    "            save_model = actor_critic\n",
    "            if arg_cuda:\n",
    "                save_model = copy.deepcopy(actor_critic).cpu()\n",
    "\n",
    "            save_model = [save_model.state_dict(),\n",
    "                          hasattr(train_envs.venv, 'ob_rms') and train_envs.venv.ob_rms or None]\n",
    "\n",
    "            torch.save(save_model, os.path.join(save_path, arg_env_name + \".pt\"))\n",
    "\n",
    "        total_num_steps = (j + 1) * update_factor\n",
    "\n",
    "        if j % arg_log_interval == 0 and len(episode_rewards) > 1:\n",
    "            end = time.time()\n",
    "            print(\"Updates {}, num timesteps {}, FPS {}, last {} mean/median reward {:.1f}/{:.1f}, \"\n",
    "                  \"min / max reward {:.1f}/{:.1f}, value/action loss {:.5f}/{:.5f}\".\n",
    "                  format(j, total_num_steps,\n",
    "                         int(total_num_steps / (end - start)),\n",
    "                         len(episode_rewards),\n",
    "                         np.mean(episode_rewards),\n",
    "                         np.median(episode_rewards),\n",
    "                         np.min(episode_rewards),\n",
    "                         np.max(episode_rewards), dist_entropy,\n",
    "                         value_loss, action_loss), end=', ' if other_metrics else '\\n')\n",
    "            with open(\"train_results_\" + arg_algo + \".txt\", \"a+\") as res_file:\n",
    "                to_print = \"{},{}\\n\".format(total_num_steps, np.mean(episode_rewards))\n",
    "                res_file.write(to_print)\n",
    "\n",
    "        if arg_eval_interval and len(episode_rewards) > 1 and j > 0 and j % arg_eval_interval == 0:\n",
    "            eval_episode_rewards = []\n",
    "\n",
    "            obs = eval_envs.reset()\n",
    "            eval_masks = torch.zeros(arg_num_processes, 1, device=device)\n",
    "\n",
    "            while len(eval_episode_rewards) < 50:\n",
    "                with torch.no_grad():\n",
    "                    _, action, _ = actor_critic.act(\n",
    "                        obs, eval_masks, deterministic=True)\n",
    "\n",
    "                # Obser reward and next obs\n",
    "                obs, reward, done, infos = eval_envs.step(action)\n",
    "                eval_masks = torch.tensor([[0.0] if done_ else [1.0] for done_ in done], device=device)\n",
    "                for info in infos:\n",
    "                    if 'episode' in info.keys():\n",
    "                        eval_episode_rewards.append(info['episode']['r'])\n",
    "            with open(\"eval_results_\" + arg_algo + \".txt\", \"a+\") as res_file:\n",
    "                to_print = \"{},{}\\n\".format(total_num_steps, np.mean(eval_episode_rewards))\n",
    "                res_file.write(to_print)\n",
    "            print(\"Evaluation using {} episodes: mean reward {:.5f}\\n\".format(len(eval_episode_rewards),\n",
    "                                                                              np.mean(eval_episode_rewards)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "If you feel like training the model yourself, feel free to uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to visualize the game play or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_hide = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the render function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def render():\n",
    "    torch.set_num_threads(1)\n",
    "    device = torch.device(\"cuda:0\" if arg_cuda else \"cpu\")\n",
    "\n",
    "    num_env = 1\n",
    "    env = make_vec_envs(arg_env_name, arg_seed + 1000,\n",
    "                        num_env, gamma=None, no_norm=arg_no_norm,\n",
    "                        num_stack=arg_num_stack, log_dir=None,\n",
    "                        device=device, eval=True, allow_early_resets=False)\n",
    "\n",
    "    # Get a render function\n",
    "    render_func = None\n",
    "    tmp_env = env\n",
    "    while True:\n",
    "        if hasattr(tmp_env, 'envs'):\n",
    "            render_func = tmp_env.envs[0].render\n",
    "            break\n",
    "        elif hasattr(tmp_env, 'venv'):\n",
    "            tmp_env = tmp_env.venv\n",
    "        elif hasattr(tmp_env, 'env'):\n",
    "            tmp_env = tmp_env.env\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # We need to use the same statistics for normalization as used in training\n",
    "    state_dict, ob_rms = torch.load(arg_load_path)\n",
    "\n",
    "    actor_critic = create_policy(\n",
    "        env.observation_space,\n",
    "        nn_kwargs={\n",
    "            'batch_norm': True,\n",
    "            'hidden_size': 512,\n",
    "        },\n",
    "        train=False)\n",
    "\n",
    "    actor_critic.load_state_dict(state_dict)\n",
    "    actor_critic.to(device)\n",
    "\n",
    "    masks = torch.zeros(num_env, 1).to(device)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    if arg_hide:\n",
    "        render_func = None\n",
    "\n",
    "    if render_func is not None:\n",
    "        render_func('human')\n",
    "\n",
    "    if arg_env_name.find('Bullet') > -1:\n",
    "        import pybullet as p\n",
    "\n",
    "        torsoId = -1\n",
    "        for i in range(p.getNumBodies()):\n",
    "            if p.getBodyInfo(i)[0].decode() == \"torso\":\n",
    "                torsoId = i\n",
    "\n",
    "    rewards = []\n",
    "    wins = 0\n",
    "    deaths = 0\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        step = step + 1\n",
    "        with torch.no_grad():\n",
    "            value, action, _ = actor_critic.act(\n",
    "                obs, masks, deterministic=True)\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "\n",
    "        masks = torch.tensor([[0.0] if done_ else [1.0] for done_ in done]).to(device)\n",
    "\n",
    "        if arg_env_name.find('Bullet') > -1:\n",
    "            if torsoId > -1:\n",
    "                distance = 5\n",
    "                yaw = 0\n",
    "                humanPos, humanOrn = p.getBasePositionAndOrientation(torsoId)\n",
    "                p.resetDebugVisualizerCamera(distance, yaw, -20, humanPos)\n",
    "\n",
    "        for i, d in enumerate(done):\n",
    "            if d:\n",
    "                rewards.append(reward[i].item())\n",
    "                if reward[i].item() > 0:\n",
    "                    wins = wins + 1\n",
    "                if reward[i].item() < 0 and step <= 800:\n",
    "                    deaths = deaths + 1\n",
    "                print(\"Game ended in {} steps, total games played: {}. Win rate: {}. Survival rate {}\".format(step-1, len(rewards), float(wins) / len(rewards), 1.0-float(deaths)/len(rewards)))\n",
    "                step = 0\n",
    "\n",
    "        if render_func is not None:\n",
    "            render_func('human')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
